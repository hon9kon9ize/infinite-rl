from typing import Tuple, Optional, Dict, Any
from .reward_function import RewardFunction
from ..executor import RewardExecutor
import json
import re


class CodingRewardFunction(RewardFunction):
    """Reward function for evaluating LLM-generated code solutions."""

    def __init__(self, task_name: str = "coding"):
        super().__init__(task_name)
        self.executor = None
        self.language = "python"  # default language

    def initialize(self):
        """Initialize the executor for running code."""
        self.executor = RewardExecutor(timeout=5)
        self.initialized = True

    def set_language(self, language: str):
        """Set the programming language for code execution."""
        supported_langs = [
            "python",
            "javascript",
            "js",
            "typescript",
            "ts",
            "cpp",
            "c++",
            "rust",
            "java",
        ]
        if language.lower() not in supported_langs:
            raise ValueError(f"Unsupported language: {language}")
        self.language = language.lower()

    def compute_reward(
        self,
        model_output: str,
        reference_answer: str,
        test_cases: Optional[list] = None,
        expected_output: Optional[str] = None,
    ) -> Tuple[float, Dict[str, Any]]:
        """
        Compute reward for generated code.

        Args:
            model_output: The code generated by the LLM
            reference_answer: The reference/correct implementation
            test_cases: Optional list of test case dicts with 'input' and 'expected_output'
            expected_output: Optional expected output for direct comparison

        Returns:
            Tuple of (reward_score, details_dict)
        """
        if not self.initialized:
            self.initialize()

        details = {
            "execution_success": False,
            "output_match": False,
            "error": None,
            "model_output": None,
            "expected_output": expected_output,
            "test_results": [],
        }

        # Try to execute the generated code
        try:
            stdout, stderr = self.executor.run_single(model_output, self.language)

            if stderr:
                details["error"] = stderr
                return 0.0, details

            details["execution_success"] = True
            details["model_output"] = stdout

            # Check if output matches expected output
            if expected_output is not None:
                if stdout == expected_output.strip():
                    details["output_match"] = True
                    return 1.0, details
                else:
                    # Partial credit based on similarity
                    similarity = self._compute_similarity(stdout, expected_output)
                    return similarity, details

            # If test cases are provided, run them
            if test_cases:
                return self._evaluate_with_test_cases(model_output, test_cases, details)

            # If no expected output or test cases, just reward for execution success
            return 0.5, details

        except Exception as e:
            details["error"] = str(e)
            return 0.0, details

    def _evaluate_with_test_cases(
        self, code: str, test_cases: list, details: Dict[str, Any]
    ) -> Tuple[float, Dict[str, Any]]:
        """Evaluate code against multiple test cases."""
        passed = 0
        total = len(test_cases)

        for test_case in test_cases:
            test_input = test_case.get("input", "")
            expected = test_case.get("expected_output", "")

            # Prepare code with test input (for Python/JS/TS)
            if self.language == "python":
                test_code = f"{code}\n# Test\nprint({test_input})"
            elif self.language in ["javascript", "js"]:
                test_code = f"{code}\n// Test\nconsole.log({test_input})"
            elif self.language in ["typescript", "ts"]:
                test_code = f"{code}\n// Test\nconsole.log({test_input})"
            else:
                # For other languages, assume code takes input from stdin
                test_code = code

            stdout, stderr = self.executor.run_single(test_code, self.language)

            test_result = {
                "input": test_input,
                "expected": expected,
                "got": stdout,
                "passed": False,
                "error": stderr,
            }

            if not stderr and stdout == expected.strip():
                test_result["passed"] = True
                passed += 1

            details["test_results"].append(test_result)

        reward = passed / total if total > 0 else 0.0
        return reward, details

    def _compute_similarity(self, output1: str, output2: str) -> float:
        """Compute simple similarity between two outputs (0-1)."""
        output1 = output1.strip().lower()
        output2 = output2.strip().lower()

        if output1 == output2:
            return 1.0

        # Check if one contains the other
        if output1 in output2 or output2 in output1:
            return 0.7

        # Check word overlap
        words1 = set(output1.split())
        words2 = set(output2.split())
        if words1 and words2:
            overlap = len(words1 & words2) / len(words1 | words2)
            return overlap * 0.5

        return 0.0
