from typing import Tuple, Optional, Dict, Any, Union, Callable
from .reward_function import RewardFunction
from ..executor import RewardExecutor
import json
import re


class CodingRewardFunction(RewardFunction):
    """Reward function for evaluating LLM-generated code solutions."""

    def __init__(self, task_name: str = "coding"):
        super().__init__(task_name)
        self.executor = None
        self.language = "python"  # default language

    def initialize(self):
        """Initialize the executor for running code."""
        self.executor = RewardExecutor(timeout=5)
        self.initialized = True

    def set_language(self, language: str):
        """Set the programming language for code execution."""
        supported_langs = [
            "python",
            "javascript",
            "js",
            "typescript",
            "ts",
            "cpp",
            "c++",
            "rust",
            "java",
        ]
        if language.lower() not in supported_langs:
            raise ValueError(f"Unsupported language: {language}")
        self.language = language.lower()

    def compute_reward(
        self,
        model_output: str,
        reference_answer: Union[str, int, Callable],
        test_cases: Optional[list] = None,
        expected_output: Optional[str] = None,
    ) -> Tuple[float, Dict[str, Any]]:
        """
        Compute reward for generated code.

        Args:
            model_output: The code generated by the LLM
            reference_answer: The reference/correct implementation (str, int, or Callable)
                - str: compare output strings
                - int: compare output as integer
                - Callable: call function with output to validate
            test_cases: Optional list of test case dicts with 'input' and 'expected_output'
            expected_output: Optional expected output for direct comparison

        Returns:
            Tuple of (reward_score, details_dict)
        """
        if not self.initialized:
            self.initialize()

        details = {
            "execution_success": False,
            "output_match": False,
            "error": None,
            "model_output": None,
            "expected_output": expected_output,
            "test_results": [],
            "reference_type": type(reference_answer).__name__,
        }

        # Try to execute the generated code
        try:
            stdout, stderr = self.executor.run_single(model_output, self.language)

            if stderr:
                details["error"] = stderr
                return 0.0, details

            details["execution_success"] = True
            details["model_output"] = stdout

            # Handle different reference_answer types
            if callable(reference_answer):
                # Callable: pass output to the function and check result
                try:
                    result = reference_answer(stdout)
                    if isinstance(result, bool):
                        score = 1.0 if result else 0.0
                        details["output_match"] = result
                    elif isinstance(result, float):
                        score = result
                        details["output_match"] = result >= 0.5
                    else:
                        score = 0.0
                        details["output_match"] = False
                    return score, details
                except Exception as e:
                    details["error"] = f"Error executing validator: {str(e)}"
                    return 0.0, details

            elif isinstance(reference_answer, int):
                # Int: try to parse output as int and compare
                try:
                    output_int = int(stdout.strip())
                    if output_int == reference_answer:
                        details["output_match"] = True
                        return 1.0, details
                    else:
                        # Partial credit based on difference
                        diff = abs(output_int - reference_answer)
                        similarity = max(
                            0.0, 1.0 - (diff / max(reference_answer, output_int))
                        )
                        return similarity, details
                except ValueError:
                    details["error"] = f"Could not parse output '{stdout}' as integer"
                    return 0.0, details

            else:
                # String: use existing string comparison logic
                if expected_output is not None:
                    if stdout == expected_output.strip():
                        details["output_match"] = True
                        return 1.0, details
                    else:
                        similarity = self._compute_similarity(stdout, expected_output)
                        return similarity, details

                # Compare with reference_answer if it's a string
                ref_str = str(reference_answer).strip()
                if stdout == ref_str:
                    details["output_match"] = True
                    return 1.0, details
                else:
                    similarity = self._compute_similarity(stdout, ref_str)
                    return similarity, details

        except Exception as e:
            details["error"] = str(e)
            return 0.0, details

    def _evaluate_with_test_cases(
        self, code: str, test_cases: list, details: Dict[str, Any]
    ) -> Tuple[float, Dict[str, Any]]:
        """Evaluate code against multiple test cases."""
        passed = 0
        total = len(test_cases)

        for test_case in test_cases:
            test_input = test_case.get("input", "")
            expected = test_case.get("expected_output", "")

            # Prepare code with test input (for Python/JS/TS)
            if self.language == "python":
                test_code = f"{code}\n# Test\nprint({test_input})"
            elif self.language in ["javascript", "js"]:
                test_code = f"{code}\n// Test\nconsole.log({test_input})"
            elif self.language in ["typescript", "ts"]:
                test_code = f"{code}\n// Test\nconsole.log({test_input})"
            else:
                # For other languages, assume code takes input from stdin
                test_code = code

            stdout, stderr = self.executor.run_single(test_code, self.language)

            test_result = {
                "input": test_input,
                "expected": expected,
                "got": stdout,
                "passed": False,
                "error": stderr,
            }

            if not stderr and stdout == expected.strip():
                test_result["passed"] = True
                passed += 1

            details["test_results"].append(test_result)

        reward = passed / total if total > 0 else 0.0
        return reward, details

    def _compute_similarity(self, output1: str, output2: str) -> float:
        """Compute simple similarity between two outputs (0-1)."""
        output1 = output1.strip().lower()
        output2 = output2.strip().lower()

        if output1 == output2:
            return 1.0

        # Check if one contains the other
        if output1 in output2 or output2 in output1:
            return 0.7

        # Check word overlap
        words1 = set(output1.split())
        words2 = set(output2.split())
        if words1 and words2:
            overlap = len(words1 & words2) / len(words1 | words2)
            return overlap * 0.5

        return 0.0

    def _evaluate_with_test_cases(
        self, code: str, test_cases: list, details: Dict[str, Any]
    ) -> Tuple[float, Dict[str, Any]]:
        """Evaluate code against multiple test cases."""
        passed = 0
        total = len(test_cases)

        for test_case in test_cases:
            test_input = test_case.get("input", "")
            expected = test_case.get("expected_output", "")

            # Prepare code with test input (for Python/JS/TS)
            if self.language == "python":
                test_code = f"{code}\n# Test\nprint({test_input})"
            elif self.language in ["javascript", "js"]:
                test_code = f"{code}\n// Test\nconsole.log({test_input})"
            elif self.language in ["typescript", "ts"]:
                test_code = f"{code}\n// Test\nconsole.log({test_input})"
            else:
                # For other languages, assume code takes input from stdin
                test_code = code

            stdout, stderr = self.executor.run_single(test_code, self.language)

            test_result = {
                "input": test_input,
                "expected": expected,
                "got": stdout,
                "passed": False,
                "error": stderr,
            }

            if not stderr and stdout == expected.strip():
                test_result["passed"] = True
                passed += 1

            details["test_results"].append(test_result)

        reward = passed / total if total > 0 else 0.0
        return reward, details

    def _compute_similarity(self, output1: str, output2: str) -> float:
        """Compute simple similarity between two outputs (0-1)."""
        output1 = output1.strip().lower()
        output2 = output2.strip().lower()

        if output1 == output2:
            return 1.0

        # Check if one contains the other
        if output1 in output2 or output2 in output1:
            return 0.7

        # Check word overlap
        words1 = set(output1.split())
        words2 = set(output2.split())
        if words1 and words2:
            overlap = len(words1 & words2) / len(words1 | words2)
            return overlap * 0.5

        return 0.0
